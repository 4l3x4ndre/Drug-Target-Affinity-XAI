{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b222535-abd0-4a0b-bd08-594101072ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from MGraphDTA.regression.preprocessing import GNNDatasetFull, GNNDataset\n",
    "from xai_dta.utils.models import load_model\n",
    "from xai_dta.config import PROJ_ROOT\n",
    "from src.utils.plot_utils import get_styled_figure_ax, style_legend\n",
    "\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ac3e16-e112-42c2-9142-5f27eb02e72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import src.utils.plot_utils\n",
    "importlib.reload(src.utils.plot_utils)\n",
    "from src.utils.plot_utils import get_styled_figure_ax, style_legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602e97e2-95c8-4f8a-9c98-d60b8d98e544",
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cpu'\n",
    "dataset_name='kiba'\n",
    "split='test'\n",
    "\n",
    "root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "model = load_model(\n",
    "    PROJ_ROOT, \n",
    "    model_name='kiba/epoch-2578, loss-0.0122, cindex-0.9741, test_loss-0.1265.pt'\n",
    ")\n",
    "dataset_path = os.path.join(PROJ_ROOT, \"MGraphDTA\", \"regression\", \"data\", dataset_name)\n",
    "dataset = GNNDataset(\n",
    "    root = dataset_path,\n",
    "    train=split=='train',\n",
    "    transform_unique=True\n",
    ")\n",
    "max_sample_train = len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e161b7b1-744a-4a93-bde4-0691ac50c00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = pd.read_csv(os.path.join(dataset_path, 'raw', f\"data_{split}.csv\"))\n",
    "\n",
    "embeddings = np.zeros((max_sample_train, 192))\n",
    "labels = np.zeros((max_sample_train, 2)) \n",
    "preds = np.zeros((max_sample_train, 1)) \n",
    "columns = ['x', 'y', 'pred', 'gt', 'compound_iso_smiles','target_sequence']\n",
    "\n",
    "indices = np.random.choice(len(dataset), max_sample_train, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c76372-178e-4b78-947a-c5d42ec8c07e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f51c992-a8bf-4ec2-9f55-e325fa3dbd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bf915c-6ed3-4573-be09-e046de5deec6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "for idx in tqdm(indices):\n",
    "    smi, seq = data_raw.iloc[idx,:][['compound_iso_smiles', 'target_sequence']]\n",
    "    data, _ = dataset.transform_unique(smi, seq)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out, protein_emb, ligand_emb = model.forward_features(data)\n",
    "        # each: [[...]]\n",
    "    \n",
    "    embedding = torch.hstack((protein_emb, ligand_emb)).cpu().numpy()[0]\n",
    "    # [bs, 192]\n",
    "\n",
    "    embeddings[i] = embedding\n",
    "    labels[i] = np.array((f\"{out.item():.2f}\", f\"{data_raw.loc[idx,'affinity']:.2f}\"))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7646366f-fa2d-444b-9d0f-ee0ab68be738",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(dataset.graph_dict.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2b9fec-7449-4806-bea3-8cb05a835872",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.shape, labels.shape, indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ba7c52-05fd-434c-85cc-2485c6dedcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f'embeddings_train_kiba_{max_sample_train}_2578.npy'\n",
    "folder = 'results/latent_space'\n",
    "os.makedirs(folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7ea7c6-57b8-450a-83e0-977bee082a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(folder, filename), embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de39d19f-df51-4441-9d84-68dcadfc94e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenamelabel = f'labels_train_kiba_{max_sample_train}_2578.npy'\n",
    "filenameindices = f'indices_train_kiba_{max_sample_train}_2578.npy'\n",
    "folder = 'results/latent_space'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7854a0c-f904-415c-acd2-383eccca732e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(folder, filenamelabel), labels)\n",
    "np.save(os.path.join(folder, filenameindices), indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8867ad0-a216-44d3-845e-c488e368cbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.load(os.path.join(folder, filename))\n",
    "labels = np.load(os.path.join(folder, filenamelabel))\n",
    "labels = np.load(os.path.join(folder, filenamelabel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8765ce-01bf-47e2-b722-3aa25ef734d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(labels[:,0], bins=50)\n",
    "plt.title('Distribution of Predicted affinity')\n",
    "plt.xlabel('Affinity')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "\n",
    "sns.histplot(labels[:,1], bins=50)\n",
    "plt.title('Distribution of True affinity')\n",
    "plt.xlabel('Affinity')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca376696-90c4-4834-b86a-6f06ac513f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_sne = TSNE(\n",
    "    n_components=2,\n",
    "    perplexity=30,\n",
    "    init=\"random\",\n",
    "    max_iter=250,\n",
    "    random_state=0,\n",
    ")\n",
    "pts_t_sne = t_sne.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230e5b6d-75f0-415d-8cab-83a33adedbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.column_stack(\n",
    "    (pts_t_sne[:,0], \n",
    "     pts_t_sne[:,1], \n",
    "     labels[:,0], \n",
    "     labels[:,1], \n",
    "     data_raw.loc[indices,'target_sequence'].to_numpy(),\n",
    "     data_raw.loc[indices,'compound_iso_smiles'].to_numpy()))\n",
    "df = pd.DataFrame(data, columns=columns)  \n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1e48a7-d607-46f6-8952-b6c42f92fe6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(os.path.join(folder, 'tsne_embeddings.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95017a5-5f41-415e-999c-ba8ea421d6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "sns.scatterplot(df, x='x', y='y', hue='pred')#, style='gt')\n",
    "plt.show()\n",
    "plt.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13345443-0877-40ce-aa87-d8da911551b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206c7aee-18f2-4a6f-9449-413365a25e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(pca_result[:, 0], pca_result[:, 1], c=labels[:,0], cmap='viridis')\n",
    "plt.title(f\"PCA - var explained: {sum(pca.explained_variance_ratio_):.2f}\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1a9f14-f7cc-41d5-943f-31d0fbc3319e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap.umap_ as umap_learn\n",
    "\n",
    "print(\"Running UMAP on the 192D interaction space...\")\n",
    "reducer = umap_learn.UMAP(n_components=2, random_state=42, n_neighbors=15, min_dist=0.1)\n",
    "\n",
    "embedding_2d = reducer.fit_transform(embeddings)\n",
    "print(\"UMAP complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cfd5b8-8de2-413a-b038-a89ff2af6677",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4. Create the plot\n",
    "plot_df = pd.DataFrame(embedding_2d, columns=['UMAP 1', 'UMAP 2'])\n",
    "plot_df['label'] = data_raw.loc[indices,'compound_iso_smiles'].to_numpy()\n",
    "\n",
    "print(\"Generating plot...\")\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(\n",
    "    data=plot_df,\n",
    "    x='UMAP 1',\n",
    "    y='UMAP 2',\n",
    "    hue='label',\n",
    "    #palette='Set1',\n",
    "    s=5,\n",
    "    alpha=0.6\n",
    ")\n",
    "\n",
    "plt.title('UMAP\"', fontsize=16)\n",
    "plt.xlabel('UMAP Dimension 1')\n",
    "plt.ylabel('UMAP Dimension 2')\n",
    "#plt.legend(title='Rule Family (Cluster)', markerscale=2)\n",
    "plt.legend('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44528460-765c-49ad-a1f4-331cc16d4c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4. Create the plot\n",
    "plot_df = pd.DataFrame(embedding_2d, columns=['UMAP 1', 'UMAP 2'])\n",
    "plot_df['label'] = labels[:,1] # gt\n",
    "\n",
    "print(\"Generating plot...\")\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(\n",
    "    data=plot_df,\n",
    "    x='UMAP 1',\n",
    "    y='UMAP 2',\n",
    "    hue='label',\n",
    "    palette='viridis',\n",
    "    s=5,\n",
    "    alpha=0.6\n",
    ")\n",
    "\n",
    "plt.title('UMAP', fontsize=16)\n",
    "plt.xlabel('UMAP Dimension 1')\n",
    "plt.ylabel('UMAP Dimension 2')\n",
    "#plt.legend(title='Rule Family (Cluster)', markerscale=2)\n",
    "plt.legend('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a2dc38-9a6b-4bf0-93a7-4a6996fc676a",
   "metadata": {},
   "source": [
    "# Modal-specific embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c87211-6e5a-46aa-b9a5-1dd1d96987e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(max_sample_train, embeddings.shape, embeddings[0].shape)\n",
    "embeddings_split = embeddings.reshape(max_sample_train * 2, 96)\n",
    "# Rows alternating: protein_0, ligand_0, protein_1, ligand_1, ...\n",
    "\n",
    "labels_split = np.repeat(labels, 2, axis=0)\n",
    "\n",
    "protein_embeddings = embeddings_split[0::2]  # Even indices: 0, 2, 4, ...\n",
    "ligand_embeddings = embeddings_split[1::2]   # Odd indices: 1, 3, 5, ...\n",
    "\n",
    "# Extract predictions and ground truth (every other row since they're duplicated)\n",
    "predictions = labels_split[0::2, 0].astype(float)  # First column, even rows\n",
    "ground_truth = labels_split[0::2, 1].astype(float)  # Second column, even rows\n",
    "\n",
    "ts = data_raw.loc[indices, 'target_sequence'].to_numpy()\n",
    "cis = data_raw.loc[indices, 'compound_iso_smiles']\n",
    "df_prot_lig = pd.DataFrame({\n",
    "    'protein_emb': list(protein_embeddings),  # Each element is a 96-dim array\n",
    "    'ligand_emb': list(ligand_embeddings),    # Each element is a 96-dim array\n",
    "    'pred': predictions,\n",
    "    'gt': ground_truth,\n",
    "    'target_sequence':ts,\n",
    "    'compound_iso_smiles':data_raw.loc[indices, 'compound_iso_smiles'].to_numpy(),\n",
    "    'seq_len': data_raw.loc[indices, 'target_sequence'].str.len(),\n",
    "    'num_C': cis.apply(lambda s: s.lower().count('c')),\n",
    "    'num_O': cis.apply(lambda s: s.lower().count('o')),\n",
    "    'num_N': cis.apply(lambda s: s.lower().count('n')),\n",
    "    'num_H': cis.apply(lambda s: s.lower().count('h')),\n",
    "    'num_F': cis.apply(lambda s: s.lower().count('f')),\n",
    "    'seq': ts\n",
    "})\n",
    "\n",
    "print(df_prot_lig.shape)\n",
    "print(df_prot_lig.head())\n",
    "print(\"Unique sequences:\",len(df_prot_lig.target_sequence.unique()))\n",
    "print(\"Unique smiles:\",len(df_prot_lig.compound_iso_smiles.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a78235-4b52-4409-80e8-9db487c4f7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Unique sequences:\",len(df_prot_lig.target_sequence.unique()))\n",
    "print(\"Unique smiles:\",len(df_prot_lig.compound_iso_smiles.unique()))\n",
    "print(\"Total df len:\", len(df_prot_lig))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67744ba-4dc1-45ff-8775-f45b4f568bea",
   "metadata": {},
   "source": [
    "## t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96bed0e-bcc2-4c52-8553-830c6acda04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_split = embeddings.reshape(max_sample_train * 2, 96)\n",
    "t_sne = TSNE(\n",
    "    n_components=2,\n",
    "    perplexity=30,\n",
    "    init=\"random\",\n",
    "    max_iter=250,\n",
    "    random_state=0,\n",
    ")\n",
    "pts_t_sne = t_sne.fit_transform(embeddings_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be7ac66-c012-46cc-aa62-0c1d38d94b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_repeated = np.repeat(labels, 2, axis=0)           # shape (2*n_samples, 2)\n",
    "seq_id_repeated = np.repeat(data_raw.loc[indices, 'target_sequence'].to_numpy(), 2)  # shape (2*n_samples,)\n",
    "smi_id_repeated = np.repeat(data_raw.loc[indices, 'compound_iso_smiles'].to_numpy(), 2)  # shape (2*n_samples,)\n",
    "data_split_tsne = np.column_stack((\n",
    "    pts_t_sne[:, 0],       # t-SNE x\n",
    "    pts_t_sne[:, 1],       # t-SNE y\n",
    "    labels_repeated,\n",
    "    seq_id_repeated,\n",
    "    smi_id_repeated \n",
    "))\n",
    "#print(data_split[:10])\n",
    "df_split_tsne = pd.DataFrame(data_split_tsne, columns=columns)\n",
    "half_labels = np.tile([\"protein\", \"ligand\"], max_sample_train)\n",
    "df_split_tsne[\"type\"] = half_labels\n",
    "\n",
    "print(df_split_tsne.describe())\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "sns.scatterplot(df_split_tsne, x='x', y='y', hue='pred')#, style='gt')\n",
    "plt.show()\n",
    "plt.close()   \n",
    "\n",
    "\n",
    "plt.figure()\n",
    "sns.scatterplot(df_split_tsne, x='x', y='y', hue='gt')#, style='gt')\n",
    "plt.show()\n",
    "plt.close()   \n",
    "\n",
    "plt.figure()\n",
    "sns.scatterplot(df_split_tsne, x='x', y='y', hue='type')#, style='gt')\n",
    "plt.show()\n",
    "plt.close()   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11090870-148a-4a7a-8d18-2dbcd8cb4c98",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c082170-1eb8-4242-a81e-9e621151abbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prot_lig\n",
    "pca = PCA(n_components=2)\n",
    "pts_pca = pca.fit_transform(\n",
    "    df_prot_lig['protein_emb'].to_list() + df_prot_lig['ligand_emb'].to_list()\n",
    ")\n",
    "df_prot_lig_pca = pd.DataFrame({\n",
    "    'x': pts_pca[:, 0],\n",
    "    'y': pts_pca[:, 1],\n",
    "    'pred': df_prot_lig['pred'].to_list() + df_prot_lig['pred'].to_list(),\n",
    "    'gt': df_prot_lig['gt'].to_list() + df_prot_lig['gt'].to_list(),\n",
    "    'type': ['protein_emb']*max_sample_train + ['ligand_emb']*max_sample_train\n",
    "})\n",
    "sns.scatterplot(df_prot_lig_pca, x='x', y='y', hue='pred')\n",
    "plt.show()\n",
    "sns.scatterplot(df_prot_lig_pca, x='x', y='y', hue='type')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b1614c-7aa0-4b22-a305-3f84fd3fbf6e",
   "metadata": {},
   "source": [
    "### Explaining PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f75ec4-5513-4365-af45-047e2a87c2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "loadings = pd.Series(pca.components_[0], index=[f\"f{i}\" for i in range(96)])\n",
    "top_pos = loadings.sort_values(ascending=False).head(10)\n",
    "top_neg = loadings.sort_values(ascending=True).head(10)\n",
    "print(\"Top positive features (x≈+20):\")\n",
    "print(top_pos)\n",
    "print(\"\\nTop negative features (x≈–20):\")\n",
    "print(top_neg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e4bc50-6591-4904-8a4a-55b126f81f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.corrcoef(pts_pca[:,0].astype(float), labels_repeated[:,0].astype(float))[0,1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d9ee2b-df5b-4343-9b0f-fb29d7b95118",
   "metadata": {},
   "source": [
    "If correlation is small (≈0), then PCA isn’t separating by prediction/affinity — it’s likely just structural (protein vs ligand).\n",
    "\n",
    "If correlation is large, then your model embeddings actually encode affinity along that direction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f546a0-b0e6-4714-b6d9-974479f4d6a5",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Plan\n",
    "\n",
    "- Separate the analyses — treat protein and ligand embeddings independently.\n",
    "\n",
    "- Run PCA/UMAP/t-SNE per-type to expose meaningful structure inside each modality.\n",
    "\n",
    "- Inspect PCA loadings to find which embedding dims matter.\n",
    "\n",
    "- Probe embeddings with simple supervised probes (linear regression/classifier) to see which dimensions predict affinity/pred.\n",
    "\n",
    "- Use attribution/perturbation on the base model (saliency, integrated gradients, SHAP on downstream probes) to tie embedding dims back to input features (sequence tokens / SMILES substructures).\n",
    "\n",
    "- Report & visualize: per-sequence and per-compound results, correlation tables, and representative examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9317cb85-9c4b-4fad-9502-076423d7afa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Split and run pca separately\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "prot = np.stack(df_prot_lig['protein_emb'].to_numpy())\n",
    "lig  = np.stack(df_prot_lig['ligand_emb'].to_numpy())\n",
    "print(prot.shape,lig.shape)\n",
    "\n",
    "pca_prot = PCA(n_components=2).fit_transform(prot)\n",
    "pca_lig  = PCA(n_components=2).fit_transform(lig)\n",
    "\n",
    "\n",
    "# plot side-by-side\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "sns.scatterplot(x=pca_prot[:,0], y=pca_prot[:,1], hue=df_prot_lig['pred'], palette=\"viridis\")\n",
    "plt.title(\"Protein PCA (colored by prediction)\")\n",
    "plt.subplot(1,2,2)\n",
    "sns.scatterplot(x=pca_lig[:,0], y=pca_lig[:,1], hue=df_prot_lig['pred'], palette=\"viridis\")\n",
    "plt.title(\"Ligand PCA (colored by prediction)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d70160-c6c6-4a35-9cfa-27454d1dd133",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=5).fit(prot) \n",
    "loadings = pd.Series(pca.components_[0], index=[f\"p{i}\" for i in range(96)])\n",
    "print(\"Top positive contributors to PC1 (protein):\\n\", loadings.nlargest(10))\n",
    "print(\"\\nTop negative contributors to PC1 (protein):\\n\", loadings.nsmallest(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f6ab91-6b0a-4f27-bcd4-c5c2ddba9d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=5).fit(lig)\n",
    "loadings = pd.Series(pca.components_[0], index=[f\"p{i}\" for i in range(96)])\n",
    "print(\"Top positive contributors to PC1 (ligand):\\n\", loadings.nlargest(10))\n",
    "print(\"\\nTop negative contributors to PC1 (ligand):\\n\", loadings.nsmallest(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8078c83b-09d4-4818-9830-ff9061de99b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as ss\n",
    "pc1_prot = pca_prot[:,0]\n",
    "r_pred_prot = ss.pearsonr(pc1_prot, df_prot_lig['pred'])[0]\n",
    "r_gt_prot   = ss.pearsonr(pc1_prot, df_prot_lig['gt'])[0]\n",
    "print(\"Protein PC1 corr with pred, gt:\", r_pred_prot, r_gt_prot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56840659-25de-4895-8cf1-fba9f9292180",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as ss\n",
    "pc1_lig = pca_lig[:,0]\n",
    "r_pred_lig = ss.pearsonr(pc1_lig, df_prot_lig['pred'])[0]\n",
    "r_gt_lig   = ss.pearsonr(pc1_lig, df_prot_lig['gt'])[0]\n",
    "print(\"Ligand PC1 corr with pred, gt:\", r_pred_lig, r_gt_lig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cae73c-fbd3-448b-be7a-990df1e2ff54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#  probe protein embeddings\n",
    "ridge = RidgeCV(alphas=np.logspace(-6,6,13), cv=5)\n",
    "scores = cross_val_score(ridge, prot, df_prot_lig['gt'], scoring=\"neg_mean_squared_error\", cv=5)\n",
    "print(\"Protein probe MSE (CV):\", -scores.mean())\n",
    "\n",
    "# Fit and inspect coefficients\n",
    "ridge.fit(prot, df_prot_lig['gt'])\n",
    "coef = pd.Series(ridge.coef_, index=[f\"p{i}\" for i in range(96)]).sort_values(key=abs, ascending=False)\n",
    "print(\"Top dims by absolute coefficient:\\n\", coef.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1184ea0-b22d-42bd-a899-f0450212670b",
   "metadata": {},
   "source": [
    "# Embedding space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecf4e59-9dbf-43eb-8138-e731e8042ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prot_lig = pd.DataFrame({\n",
    "    'protein_emb': list(protein_embeddings),  # Each element is a 96-dim array\n",
    "    'ligand_emb': list(ligand_embeddings),    # Each element is a 96-dim array\n",
    "    'pred': predictions,\n",
    "    'gt': ground_truth,\n",
    "    'target_sequence':data_raw.loc[indices, 'target_sequence'].to_numpy(),\n",
    "    'compound_iso_smiles':data_raw.loc[indices, 'compound_iso_smiles'].to_numpy()\n",
    "})\n",
    "df_prot_lig.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c381c03-967a-48aa-95be-466df40cb068",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert lists of embeddings into arrays\n",
    "prot_emb_arr = np.stack(df_prot_lig['protein_emb'].to_numpy())  # shape (N, 96)\n",
    "lig_emb_arr  = np.stack(df_prot_lig['ligand_emb'].to_numpy())    # shape (N, 96)\n",
    "\n",
    "# Compute correlation of each embedding dim with prediction and ground truth\n",
    "prot_corr_pred = np.corrcoef(prot_emb_arr.T, df_prot_lig['pred'].to_numpy())[0:96, -1]\n",
    "prot_corr_gt   = np.corrcoef(prot_emb_arr.T, df_prot_lig['gt'].to_numpy())[0:96, -1]\n",
    "\n",
    "lig_corr_pred  = np.corrcoef(lig_emb_arr.T, df_prot_lig['pred'].to_numpy())[0:96, -1]\n",
    "lig_corr_gt    = np.corrcoef(lig_emb_arr.T, df_prot_lig['gt'].to_numpy())[0:96, -1]\n",
    "\n",
    "# Create DataFrame for heatmap\n",
    "heatmap_df = pd.DataFrame({\n",
    "    'Protein': prot_corr_pred,\n",
    "    #'protein_gt_corr': prot_corr_gt,\n",
    "    'Ligand': lig_corr_pred,\n",
    "    #'ligand_gt_corr': lig_corr_gt\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(18, 6))\n",
    "sns.heatmap(heatmap_df.T, cmap='coolwarm', center=0, annot=False, fmt=\".2f\")\n",
    "#plt.title(\"Embedding dimension correlation with prediction / ground truth\")\n",
    "plt.xlabel(\"Embedding dimension\")\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/latent_space/embeddings_correlations.svg',bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(50,6))\n",
    "sns.heatmap(heatmap_df.T, cmap='coolwarm', center=0, annot=True, fmt=\".2f\")\n",
    "plt.title(\"Embedding dimension correlation with prediction / ground truth\")\n",
    "plt.xlabel(\"Embedding dimension\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3dfc78-94c9-4bd1-a18c-986f25904bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Helper function to calculate correlations for a subgroup\n",
    "def get_seq_correlations(group_df, emb_col, target_col, n_dims=96):\n",
    "    \"\"\"\n",
    "    Calculates the correlation between embedding dimensions and a target\n",
    "    for a given subgroup DataFrame.\n",
    "    \"\"\"\n",
    "    # Create a reusable Series of NaNs\n",
    "    nan_series = pd.Series([np.nan] * n_dims, index=[f'dim_{i}' for i in range(n_dims)])\n",
    "    \n",
    "    # Find rows where both embedding and target are valid\n",
    "    valid_indices = group_df[emb_col].notna() & group_df[target_col].notna()\n",
    "    \n",
    "    # Need at least 2 samples to compute correlation\n",
    "    if valid_indices.sum() < 2:\n",
    "        return nan_series # Return all NaNs\n",
    "\n",
    "    emb_arr = np.stack(group_df.loc[valid_indices, emb_col].to_numpy())\n",
    "    target_vals = group_df.loc[valid_indices, target_col].to_numpy()\n",
    "\n",
    "    # Check for constant target value (correlation is undefined)\n",
    "    if np.std(target_vals) == 0:\n",
    "        return nan_series # Return all NaNs\n",
    "\n",
    "    # np.corrcoef handles constant embedding dims by returning NaN\n",
    "    corrs = np.corrcoef(emb_arr.T, target_vals)[0:n_dims, -1]\n",
    "    \n",
    "    return pd.Series(corrs, index=[f'dim_{i}' for i in range(n_dims)])\n",
    "\n",
    "# --- 1. Calculate correlations, grouping by sequence ---\n",
    "print(\"Calculating correlations for each sequence... This may take a moment.\")\n",
    "\n",
    "prot_pred_corr_by_seq = df_prot_lig.groupby('target_sequence').apply(\n",
    "    get_seq_correlations, emb_col='protein_emb', target_col='pred', \n",
    "    n_dims=96, include_groups=False \n",
    ")\n",
    "\n",
    "prot_gt_corr_by_seq = df_prot_lig.groupby('target_sequence').apply(\n",
    "    get_seq_correlations, emb_col='protein_emb', target_col='gt', \n",
    "    n_dims=96, include_groups=False \n",
    ")\n",
    "\n",
    "lig_pred_corr_by_seq = df_prot_lig.groupby('target_sequence').apply(\n",
    "    get_seq_correlations, emb_col='ligand_emb', target_col='pred', \n",
    "    n_dims=96, include_groups=False \n",
    ")\n",
    "\n",
    "lig_gt_corr_by_seq = df_prot_lig.groupby('target_sequence').apply(\n",
    "    get_seq_correlations, emb_col='ligand_emb', target_col='gt', \n",
    "    n_dims=96, include_groups=False\n",
    ")\n",
    "\n",
    "print(\"Calculations complete. Generating heatmaps...\")\n",
    "\n",
    "# --- 2. Plot the new heatmaps with a safer plotting block ---\n",
    "\n",
    "def plot_correlation_clustermap(corr_df, title):\n",
    "    \"\"\"Helper function to safely plot the clustermap.\"\"\"\n",
    "\n",
    "    df_to_plot = corr_df.dropna(how='all', axis=0).dropna(how='all', axis=1)\n",
    "\n",
    "    if df_to_plot.empty:\n",
    "        print(f\"SKIPPING PLOT: '{title}'\")\n",
    "        print(\"After cleaning, no valid (non-NaN) data remained to plot.\")\n",
    "        print(\"-\" * 30)\n",
    "        return\n",
    "\n",
    "    if df_to_plot.shape[0] < 2 or df_to_plot.shape[1] < 2:\n",
    "        print(f\"SKIPPING PLOT: '{title}'\")\n",
    "        print(f\"Not enough data to cluster (shape: {df_to_plot.shape}). Need at least (2, 2).\")\n",
    "        print(\"-\" * 30)\n",
    "        return\n",
    "\n",
    "    print(f\"Plotting '{title}' with shape {df_to_plot.shape}...\")\n",
    "    try:\n",
    "        g = sns.clustermap(\n",
    "            df_to_plot,\n",
    "            cmap='coolwarm',\n",
    "            center=0,\n",
    "            figsize=(12, 10),\n",
    "            cbar_kws={'label': 'Correlation'},\n",
    "            xticklabels=False,\n",
    "            yticklabels=False \n",
    "        )\n",
    "        g.fig.suptitle(title, y=1.02)\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR plotting '{title}': {e}\")\n",
    "        print(\"This can happen if data is still sparse. Skipping.\")\n",
    "        plt.close()\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# --- Generate  plots  ---\n",
    "plot_correlation_clustermap(\n",
    "    prot_pred_corr_by_seq, \n",
    "    'Protein Embedding vs. Prediction (Clustered by Sequence)'\n",
    ")\n",
    "\n",
    "plot_correlation_clustermap(\n",
    "    prot_gt_corr_by_seq, \n",
    "    'Protein Embedding vs. Ground Truth (Clustered by Sequence)'\n",
    ")\n",
    "\n",
    "plot_correlation_clustermap(\n",
    "    lig_pred_corr_by_seq, \n",
    "    'Ligand Embedding vs. Prediction (Clustered by Sequence)'\n",
    ")\n",
    "\n",
    "plot_correlation_clustermap(\n",
    "    lig_gt_corr_by_seq, \n",
    "    'Ligand Embedding vs. Ground Truth (Clustered by Sequence)'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e8eecb-a5c2-4272-a309-484c39f8ec03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Calculate the standard deviation for each dimension (column)\n",
    "\n",
    "if 'lig_gt_corr_by_seq' in locals() and not lig_gt_corr_by_seq.empty:\n",
    "    \n",
    "    # std(axis=0) calculates the stdev for each column\n",
    "    gt_variability = lig_gt_corr_by_seq.std(axis=0)\n",
    "    \n",
    "    # Sort the dimensions from most variable to least variable\n",
    "    gt_variability_sorted = gt_variability.sort_values(ascending=False)\n",
    "\n",
    "    print(\"--- Top 10 Most 'Context-Dependent' Ligand Dimensions (vs. Ground Truth) ---\")\n",
    "    print(gt_variability_sorted.head(10))\n",
    "    print(\"\\n(Dimension index is on the left, Std. Dev. of correlation is on the right)\")\n",
    "\n",
    "\n",
    "    # 2. Create a bar plot to visualize the variability of all dimensions\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    gt_variability_sorted.plot(\n",
    "        kind='bar', \n",
    "        color='mediumpurple',\n",
    "        width=0.8, # Use width to make bars touch, like a histogram\n",
    "        edgecolor='black'\n",
    "    )\n",
    "    \n",
    "    plt.title('Variability of Ligand Embedding Dimensions (vs. Ground Truth)', fontsize=16)\n",
    "    plt.ylabel('Standard Deviation of Correlation', fontsize=12)\n",
    "    plt.xlabel('Embedding Dimension (Sorted by Variability)', fontsize=12)\n",
    "    \n",
    "    plt.xticks([]) \n",
    "    \n",
    "    # Add a note about what this means\n",
    "    plt.text(0.98, 0.95, \n",
    "             'High Bar = High Variability\\n(Dim is context-dependent)', \n",
    "             ha='right', va='top', transform=plt.gca().transAxes,\n",
    "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.5))\n",
    "    \n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"Could not find the 'lig_gt_corr_by_seq' DataFrame to analyze.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8171f46d-4ba2-4dd9-8e87-8187b1853a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if ('gt_variability_sorted' not in locals() or \n",
    "    'lig_gt_corr_by_seq' not in locals() or\n",
    "    'df_prot_lig' not in locals()):\n",
    "    \n",
    "    print(\"Error: Required DataFrames ('gt_variability_sorted', 'lig_gt_corr_by_seq', 'df_prot_lig') not found.\")\n",
    "    print(\"Please re-run the previous code cells.\")\n",
    "else:\n",
    "\n",
    "    # 1. Get the ID of the most variable dimension \n",
    "    top_dim_id = gt_variability_sorted.index[0]\n",
    "    top_dim_index = int(top_dim_id.split('_')[1])\n",
    "\n",
    "    print(f\"--- Analyzing Top Variable Dimension: {top_dim_id} (index {top_dim_index}) ---\")\n",
    "\n",
    "    # Define a minimum number of samples to trust a correlation\n",
    "    MIN_SAMPLES_FOR_STATS = 10 \n",
    "    \n",
    "    # 2a. Get the sample counts for each sequence\n",
    "    sequence_counts = df_prot_lig['target_sequence'].value_counts()\n",
    "    \n",
    "    # 2b. Find which sequences meet our threshold\n",
    "    valid_sequences = sequence_counts[sequence_counts >= MIN_SAMPLES_FOR_STATS].index\n",
    "    \n",
    "    print(f\"Found {len(valid_sequences)} sequences (out of {len(sequence_counts)}) with at least {MIN_SAMPLES_FOR_STATS} samples.\")\n",
    "\n",
    "    # 2c. Get the correlations for our top dimension\n",
    "    corrs_for_top_dim = lig_gt_corr_by_seq[top_dim_id].dropna()\n",
    "    \n",
    "    # 2d. Filter the correlations to ONLY include our valid sequences\n",
    "    corrs_valid = corrs_for_top_dim.loc[corrs_for_top_dim.index.isin(valid_sequences)]\n",
    "    \n",
    "    if corrs_valid.empty:\n",
    "        print(\"\\n*** ERROR ***\")\n",
    "        print(f\"No sequences found with >= {MIN_SAMPLES_FOR_STATS} samples that also have valid correlation data.\")\n",
    "        print(\"Try lowering 'MIN_SAMPLES_FOR_STATS' (e.g., to 5) and re-running.\")\n",
    "    else:\n",
    "        # 3. Find the 3 representative sequences from the filtered list\n",
    "        \n",
    "        # Sequence with highest positive correlation\n",
    "        seq_pos_corr = corrs_valid.idxmax()\n",
    "        val_pos_corr = corrs_valid.max()\n",
    "        print(f\"Positive sequence: {seq_pos_corr[:20]}... (Corr: {val_pos_corr:.2f})\")\n",
    "        \n",
    "        # Sequence with highest negative correlation\n",
    "        seq_neg_corr = corrs_valid.idxmin()\n",
    "        val_neg_corr = corrs_valid.min()\n",
    "        print(f\"Negative sequence: {seq_neg_corr[:20]}... (Corr: {val_neg_corr:.2f})\")\n",
    "\n",
    "        # Sequence with correlation closest to zero\n",
    "        seq_zero_corr = (corrs_valid - 0).abs().idxmin()\n",
    "        val_zero_corr = corrs_valid[seq_zero_corr]\n",
    "        print(f\"Neutral sequence:  {seq_zero_corr[:20]}... (Corr: {val_zero_corr:.2f})\")\n",
    "\n",
    "        selected_sequences = [seq_pos_corr, seq_neg_corr, seq_zero_corr]\n",
    "        \n",
    "        # 4. Prepare data for plotting\n",
    "        plot_df = df_prot_lig[df_prot_lig['target_sequence'].isin(selected_sequences)].copy()\n",
    "        \n",
    "        plot_df[top_dim_id] = plot_df['ligand_emb'].apply(lambda x: x[top_dim_index] if isinstance(x, np.ndarray) else np.nan)\n",
    "        \n",
    "        type_map = {\n",
    "            seq_pos_corr: f'Positive Corr (r={val_pos_corr:.2f}, N={sequence_counts[seq_pos_corr]})',\n",
    "            seq_neg_corr: f'Negative Corr (r={val_neg_corr:.2f}, N={sequence_counts[seq_neg_corr]})',\n",
    "            seq_zero_corr: f'Neutral Corr (r={val_zero_corr:.2f}, N={sequence_counts[seq_zero_corr]})'\n",
    "        }\n",
    "        plot_df['Correlation Type'] = plot_df['target_sequence'].map(type_map)\n",
    "\n",
    "        # 5. Create the scatter plot\n",
    "        print(\"\\nGenerating new, filtered plot...\")\n",
    "        \n",
    "        g = sns.lmplot(\n",
    "            data=plot_df,\n",
    "            x=top_dim_id,\n",
    "            y='gt',\n",
    "            hue='Correlation Type',\n",
    "            height=6,\n",
    "            aspect=1.5,\n",
    "            palette='Set1',\n",
    "            scatter_kws={'alpha': 0.6, 's': 50},\n",
    "            legend_out=False\n",
    "        )\n",
    "        \n",
    "        g.set_axis_labels(f'Value of Ligand Embedding Dimension {top_dim_index}', 'Ground Truth Value')\n",
    "        plt.title(f'Context-Dependent Behavior of Dimension {top_dim_index} (Filtered)', fontsize=16)\n",
    "        plt.grid(True, linestyle='--', alpha=0.5)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b022c2b4-ee8e-4965-8f93-093dd4da98c2",
   "metadata": {},
   "source": [
    "The Plan:\n",
    "\n",
    "    Cluster the Sequences: We'll use AgglomerativeClustering to group  sequences into k \"families\"\n",
    "\n",
    "    Calculate Average Rules: We'll compute the mean correlation vector for each family.\n",
    "\n",
    "    Plot the Averages: We'll create a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17679165-bb3d-4ed5-815c-73045ac91687",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "if ('lig_pred_corr_by_seq' not in locals() or \n",
    "    'df_prot_lig' not in locals()):\n",
    "    print(\"Error: Required DataFrames not found. Please re-run earlier code.\")\n",
    "else:\n",
    "\n",
    "    # 1. Get the sample counts for each sequence\n",
    "    MIN_SAMPLES_FOR_CLUSTERING = 10 \n",
    "    sequence_counts = df_prot_lig['target_sequence'].value_counts()\n",
    "    valid_sequences = sequence_counts[sequence_counts >= MIN_SAMPLES_FOR_CLUSTERING].index\n",
    "\n",
    "    print(f\"Total sequences before filtering: {len(lig_pred_corr_by_seq)}\")\n",
    "    print(f\"Found {len(valid_sequences)} sequences with at least {MIN_SAMPLES_FOR_CLUSTERING} samples.\")\n",
    "\n",
    "    # 2. Filter the correlation data to ONLY include these valid sequences\n",
    "    valid_corr_data = lig_pred_corr_by_seq.loc[lig_pred_corr_by_seq.index.isin(valid_sequences)]\n",
    "\n",
    "    # 3. Prepare this clean data for clustering\n",
    "    data_to_cluster_clean = valid_corr_data.dropna(how='all', axis=0).fillna(0)\n",
    "    print(f\"Clustering {len(data_to_cluster_clean)} valid sequences.\")\n",
    "\n",
    "    # 4. Set up the clustering model\n",
    "    #    Let's try 3 clusters: \"Family A\", \"Family B\", \"Noisy Family\"\n",
    "    N_CLUSTERS = 3\n",
    "    model = AgglomerativeClustering(n_clusters=N_CLUSTERS)\n",
    "\n",
    "    # 5. Fit the model and get the \"family\" (cluster) labels\n",
    "    labels = model.fit_predict(data_to_cluster_clean)\n",
    "\n",
    "    # 6. Add the labels back to our clean data\n",
    "    cluster_labels = pd.Series(labels, index=data_to_cluster_clean.index, name='cluster')\n",
    "\n",
    "    # 7. Let's see how big each \"family\" is\n",
    "    print(\"\\n--- Cluster (Family) Sizes ---\")\n",
    "    print(cluster_labels.value_counts().sort_index())\n",
    "\n",
    "    # 8. Join the labels and calculate the \"average rule\"\n",
    "    data_with_labels_clean = data_to_cluster_clean.join(cluster_labels)\n",
    "    cluster_means_clean = data_with_labels_clean.groupby('cluster').mean()\n",
    "\n",
    "    # 9. Print the resulting table\n",
    "    print(\"\\n--- Average Correlation 'Rule' for each family (DataFrame) ---\")\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', 1000)\n",
    "    print(cluster_means_clean.round(2))\n",
    "\n",
    "    # 10. Plot the heatmap\n",
    "    print(\"\\n--- Plotting the 'Average Rule' for each family ---\")\n",
    "    plt.figure(figsize=(40, 3)) \n",
    "    sns.heatmap(\n",
    "        cluster_means_clean,\n",
    "        cmap='coolwarm',\n",
    "        center=0,\n",
    "        annot=True,\n",
    "        fmt=\".2f\", \n",
    "        annot_kws={\"size\": 5}\n",
    "    )\n",
    "    #plt.title(f'Average Correlation \"Rule\" for each of the {N_CLUSTERS} Families (Filtered)', fontsize=16)\n",
    "    plt.xlabel('Embedding Dimension')\n",
    "    plt.ylabel('Cluster')\n",
    "    plt.savefig('results/latent_space_families/average_correlation_rule_heatmap.svg', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.figure(figsize=(15, 3)) # 3 clusters = less height\n",
    "    sns.heatmap(\n",
    "        cluster_means_clean,\n",
    "        cmap='coolwarm',\n",
    "        center=0,\n",
    "        annot=False,\n",
    "        fmt=\".2f\", # Use 2 decimal places\n",
    "        annot_kws={\"size\": 8}\n",
    "    )\n",
    "    plt.title(f'Average Correlation \"Rule\" for each of the {N_CLUSTERS} Families (Filtered)', fontsize=16)\n",
    "    plt.xlabel('Embedding Dimension')\n",
    "    plt.ylabel('Cluster (Family) ID')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61ea5ed-1e3e-4e34-a95a-4f0792408172",
   "metadata": {},
   "source": [
    "Other runs:\n",
    "\n",
    "    2 Clusters:\n",
    "        cluster\n",
    "    0    167\n",
    "    1     26\n",
    "\n",
    "    3 Clusters: \n",
    "    cluster\n",
    "    0    160\n",
    "    1     26\n",
    "    2      7\n",
    "\n",
    "    4 Clusters: \n",
    "    cluster\n",
    "    0      7\n",
    "    1      3\n",
    "    2    157\n",
    "    3     26\n",
    "\n",
    "    5 Clusters: \n",
    "    cluster\n",
    "    0    157\n",
    "    1      3\n",
    "    2      4\n",
    "    3     26\n",
    "    4      3\n",
    "\n",
    "There is a pattern, over segmenting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73accb0-09d6-4b4d-aa65-9a156d244d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "DATASET_COLORS = ['#95BB63', '#BCBCE0', '#77b5b6', '#EA805D']\n",
    "\n",
    "def format_label_family_name(label):\n",
    "    return f'Cluster {int(label)+1}'\n",
    "\n",
    "if 'cluster_means_clean' in locals():\n",
    "    # 1. Identify the key dimensions based\n",
    "    key_dimensions = [\n",
    "        f'dim_{d}' for d in [0, 11, 19, 20, 23, 31, 35, 39, 43, 60, 74, 77]\n",
    "    ]\n",
    "    \n",
    "    # 2. Extract only these key dimensions from the means table\n",
    "    key_dim_data = cluster_means_clean[key_dimensions]\n",
    "    \n",
    "    # 3. Prepare the data for plotting (melt from wide to long)\n",
    "    key_dim_data_long = key_dim_data.reset_index().melt(\n",
    "        id_vars='cluster',\n",
    "        var_name='Dimension',\n",
    "        value_name='Correlation'\n",
    "    )\n",
    "    \n",
    "    # 4. Create the grouped bar plot\n",
    "    print(\"--- Plotting a comparison of the key 'rule' dimensions ---\")\n",
    "    #plt.figure(figsize=(18, 7))\n",
    "    fig, ax = get_styled_figure_ax(figsize=(18, 7), aspect='none')\n",
    "    sns.barplot(\n",
    "        data=key_dim_data_long,\n",
    "        x='Dimension',\n",
    "        y='Correlation',\n",
    "        hue='cluster',  # This creates the grouped bars\n",
    "        palette=DATASET_COLORS,\n",
    "    )\n",
    "    \n",
    "    #plt.title('Comparison of Key \"Rules\" Across the Three Families', fontsize=16)\n",
    "    plt.xlabel('Embedding Dimension', fontsize=12)\n",
    "    plt.ylabel('Average Correlation', fontsize=12)\n",
    "    \n",
    "    # Add a horizontal line at y=0 for reference\n",
    "    plt.axhline(0, color='black', linestyle='--', linewidth=0.8)\n",
    "    \n",
    "    #plt.legend(title='Cluster (Family) ID')\n",
    "    style_legend(ax, ncol=3, bbox_to_anchor=(0.5, 1.1), format_labels=format_label_family_name)\n",
    "\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.savefig('results/latent_space_families/key_dimensions_comparison.svg', dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    print(\"Generated plot 'key_dimensions_comparison.png'\")\n",
    "\n",
    "else:\n",
    "    print(\"Could not find 'cluster_means_clean' to plot. Please run the clustering code.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b162919-1e3b-4aa0-8e56-8efa7115b1f0",
   "metadata": {},
   "source": [
    "### The Test\n",
    "\n",
    "We can test that this is not random by simulating the \"random\" hypothesis:\n",
    "\n",
    "1.  **Calculate our \"Observed\" metric:** We'll take our `cluster_means_clean` table (the 3 \"rules\") and calculate a single number that represents how different they are. The mean of their standard deviations will be the metric: `cluster_means_clean.std().mean()`. A high value means the rules are very different.\n",
    "2.  **Run a Simulation (e.g., 1000 times):**\n",
    "    a. Take our 166 \"valid\" sequences.\n",
    "    b. **Shuffle the labels.** Instead of the real families, we'll randomly assign sequences to \"fake\" families of the same size.\n",
    "    c. Calculate the \"average rules\" for these **fake, random** families.\n",
    "    d. Calculate the metric (the variability) for these fake rules.\n",
    "3.  **Compare:** We'll end up with 1 \"Observed\" value and 1000 \"Random\" values.\n",
    "      * **If our analysis is real:** Our \"Observed\" value will be a massive outlier. The 1000 \"Random\" values will be tiny.\n",
    "      * **If our analysis is random:** Our \"Observed\" value will be lost in the middle of the 1000 \"Random\" values.\n",
    "\n",
    "The **p-value** will be the fraction of \"Random\" values that were *larger* than our \"Observed\" value. A p-value of `0.0` means our results are **not** random.\n",
    "\n",
    "\n",
    "### Output\n",
    "\n",
    "We will get two things:\n",
    "\n",
    "1.  **A p-value** \n",
    "2.  **A Histogram**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13aa14c9-f768-46c3-aba0-47990c7c1626",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "if ('data_to_cluster_clean' not in locals() or \n",
    "    'cluster_means_clean' not in locals() or\n",
    "    'cluster_labels' not in locals()):\n",
    "    \n",
    "    print(\"Error: Key data (e.g., 'data_to_cluster_clean') is missing.\")\n",
    "    print(\"This likely means the session restarted. Please re-run the clustering code first.\")\n",
    "\n",
    "else:\n",
    "    print(\"--- Running Permutation Test for Statistical Significance ---\")\n",
    "    \n",
    "    N_PERMUTATIONS = 1000\n",
    "    \n",
    "    # 1. Calculate our ONE \"Observed\" metric\n",
    "    #    We use .std() to see how different the clusters are from each other,\n",
    "    #    and .mean() to get a single number.\n",
    "    observed_metric = cluster_means_clean.std(axis=0).mean()\n",
    "    \n",
    "    print(f\"Observed Metric (Variability): {observed_metric:.4f}\")\n",
    "    \n",
    "    # This holds the correlation data\n",
    "    base_data = data_to_cluster_clean.copy() \n",
    "    \n",
    "    # This holds the labels (e.g., [0, 1, 2, 0, 1...])\n",
    "    original_labels = cluster_labels.copy()\n",
    "    \n",
    "    random_metrics = [] # We'll store 1000 \"random\" metrics here\n",
    "    \n",
    "    # 2. Run the simulation 1000 times\n",
    "    for _ in range(N_PERMUTATIONS):\n",
    "        # a. Shuffle the labels\n",
    "        shuffled_labels = np.random.permutation(original_labels)\n",
    "        \n",
    "        # b. Assign the shuffled labels to the original data\n",
    "        base_data['shuffled_cluster'] = shuffled_labels\n",
    "        \n",
    "        # c. Calculate the \"average rules\" for these FAKE random families\n",
    "        random_means = base_data.groupby('shuffled_cluster').mean()\n",
    "        \n",
    "        # d. Calculate the metric for the FAKE rules\n",
    "        random_metric = random_means.std(axis=0).mean()\n",
    "        \n",
    "        # e. Store it\n",
    "        random_metrics.append(random_metric)\n",
    "        \n",
    "    print(f\"Simulation complete. (Ran {N_PERMUTATIONS} permutations)\")\n",
    "    \n",
    "    # 3. Compare and calculate p-value\n",
    "    random_metrics = np.array(random_metrics)\n",
    "    \n",
    "    # How many \"random\" runs were > our \"observed\" one?\n",
    "    n_exceeded = np.sum(random_metrics >= observed_metric)\n",
    "    \n",
    "    p_value = n_exceeded / N_PERMUTATIONS\n",
    "    \n",
    "    print(\"\\n--- Results ---\")\n",
    "    print(f\"P-value: {p_value}\")\n",
    "    print(f\"(This is the probability of seeing our result by random chance)\")\n",
    "\n",
    "    # 4. Plot the results\n",
    "    #plt.figure(figsize=(10, 6))\n",
    "    fig, ax = get_styled_figure_ax(figsize=(15, 8), aspect='none', grid=True)\n",
    "    sns.histplot(random_metrics, label='Random Permutations (Null Hypothesis)', \n",
    "                 bins=30, kde=True, color=DATASET_COLORS[2])\n",
    "    \n",
    "    # Plot our \"Observed\" value as a big red line\n",
    "    plt.axvline(\n",
    "        observed_metric, \n",
    "        color=DATASET_COLORS[-1], \n",
    "        linewidth=3, \n",
    "        linestyle='--', \n",
    "        label=f'Observed Value: {observed_metric:.4f}'\n",
    "    )\n",
    "    \n",
    "    #plt.title('Permutation Test for Cluster Significance', fontsize=16)\n",
    "    plt.xlabel('Variability Metric (Higher = More Different)')\n",
    "    plt.ylabel('Frequency')\n",
    "    #plt.legend()\n",
    "    style_legend(ax, ncol=3, bbox_to_anchor=(0.5, 1.1))\n",
    "    plt.savefig('results/latent_space_families/permutation_test.svg')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34d4037-0e5e-48e8-b1da-3e4614c416dd",
   "metadata": {},
   "source": [
    "# Feature space of proteins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5586bc6b-19ac-4d4c-8146-bed232d8f242",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import umap.umap_ as umap_learn\n",
    "\n",
    "# --- Check if we have the necessary data ---\n",
    "if ('df_prot_lig' not in locals() or \n",
    "    'cluster_labels' not in locals()):\n",
    "    \n",
    "    print(\"Error: Key data ('df_prot_lig' or 'cluster_labels') is missing.\")\n",
    "    print(\"This likely means the session restarted.\")\n",
    "    print(\"Please re-run the analysis from the beginning (creating df_prot_lig) \")\n",
    "    print(\"and the *filtered* clustering step (creating cluster_labels).\")\n",
    "else:\n",
    "\n",
    "    # --- 1. Get the Unique Protein Embeddings ---\n",
    "    # We drop duplicates and set the sequence as the index\n",
    "    unique_prots_df = df_prot_lig.drop_duplicates(\n",
    "        subset=['target_sequence']\n",
    "    ).set_index('target_sequence')\n",
    "\n",
    "    print(f\"Found {len(unique_prots_df)} unique protein sequences.\")\n",
    "\n",
    "    # --- 2. Run UMAP to get 2D coordinates ---\n",
    "    # Stack the list of arrays into a single numpy array\n",
    "    prot_emb_array = np.stack(unique_prots_df['protein_emb'].values)\n",
    "\n",
    "    # Initialize UMAP.\n",
    "    reducer = umap_learn.UMAP(n_components=2, random_state=42, n_neighbors=15, min_dist=0.1)\n",
    "\n",
    "    print(\"Running UMAP to reduce 96D protein embeddings to 2D...\")\n",
    "    # Fit UMAP to the protein embeddings\n",
    "    embedding_2d = reducer.fit_transform(prot_emb_array)\n",
    "    print(\"UMAP complete.\")\n",
    "\n",
    "    # --- 3. Create the Final DataFrame for Plotting ---\n",
    "    # Create a new DataFrame with the 2D coordinates\n",
    "    plot_df = pd.DataFrame(\n",
    "        embedding_2d,\n",
    "        columns=['UMAP 1', 'UMAP 2'],\n",
    "        index=unique_prots_df.index # Use sequence as index\n",
    "    )\n",
    "    \n",
    "    # --- 4. Merge our 'Family' (cluster_labels) ---\n",
    "    plot_df = plot_df.join(cluster_labels)\n",
    "    \n",
    "    # Fill the 'NaN's so they show up in the plot legend\n",
    "    # These are the sequences we didn't have enough data to test\n",
    "    plot_df['cluster'] = plot_df['cluster'].fillna('Not in Test')\n",
    "    \n",
    "    # Convert cluster labels to string for categorical coloring\n",
    "    plot_df['cluster'] = plot_df['cluster'].astype(str)\n",
    "\n",
    "    # --- 5. Plot the Final Visualization ---\n",
    "    print(\"Generating plot...\")\n",
    "    #plt.figure(figsize=(12, 8))\n",
    "    fig, ax = get_styled_figure_ax(figsize=(15, 8), aspect='none', grid=True)\n",
    "    sns.scatterplot(\n",
    "        data=plot_df,\n",
    "        x='UMAP 1',\n",
    "        y='UMAP 2',\n",
    "        hue='cluster', # Color by our \"family\"\n",
    "        palette='Set1',\n",
    "        s=50,          \n",
    "        alpha=0.8\n",
    "    )\n",
    "    \n",
    "    \n",
    "    #plt.title('Protein Embeddings (UMAP) Colored by \"Rule Family\"', fontsize=16)\n",
    "    plt.xlabel('UMAP Dimension 1')\n",
    "    plt.ylabel('UMAP Dimension 2')\n",
    "    #plt.legend(title='Rule Family (Cluster)')\n",
    "    style_legend(ax, ncol=4, bbox_to_anchor=(0.5, 1.1))\n",
    "    #plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.savefig('results/latent_space_families/protein_emb_umap_col-rulefamily.svg')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ef13af-3eae-4aad-aea8-36f9c30a672e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9354bc5-871e-4447-b38c-4958a93f064f",
   "metadata": {},
   "source": [
    "# t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2705ca-2b19-4a78-a16d-5a2433fa4de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted average of outer products by prediction\n",
    "interaction_matrix = np.zeros((96, 96))\n",
    "for i in range(len(df_prot_lig)):\n",
    "    p = prot_emb_arr[i][:, None]  # (96,1)\n",
    "    l = lig_emb_arr[i][None, :]  # (1,96)\n",
    "    interaction_matrix += df_prot_lig['pred'].iloc[i] * (p @ l)  # weighted outer product\n",
    "\n",
    "# Normalize\n",
    "interaction_matrix /= len(df_prot_lig)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(interaction_matrix, cmap='coolwarm', center=0)\n",
    "plt.title(\"Protein × Ligand embedding interaction heatmap (weighted by prediction)\")\n",
    "plt.xlabel(\"Ligand embedding dim\")\n",
    "plt.ylabel(\"Protein embedding dim\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc1f619-5b38-46fd-b1e5-f075d850ab00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63cbd78-7994-438d-961d-9d9c1081687a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(interaction_matrix, \n",
    "            cmap='coolwarm', \n",
    "            center=0, \n",
    "            robust=True) # tells Seaborn to calculate the colormap range using quantiles\n",
    "            # (by default, the 2nd and 98th percentiles) instead of the \n",
    "            # absolute minimum and maximum.\n",
    "plt.title(\"Protein × Ligand interaction heatmap (robust scaling)\")\n",
    "plt.xlabel(\"Ligand embedding dim\")\n",
    "plt.ylabel(\"Protein embedding dim\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7e32bf-5a65-445e-bf98-ab4063fba67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_heatmap = np.hstack([prot_emb_arr, lig_emb_arr])  # shape (N, 192)\n",
    "sns.heatmap(sample_heatmap, cmap='coolwarm', center=0)\n",
    "plt.title(\"All samples embedding heatmap\")\n",
    "plt.xlabel(\"Embedding dimensions (protein + ligand)\")\n",
    "plt.ylabel(\"Samples\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262c4270-1267-4c7b-a048-6407f8691546",
   "metadata": {},
   "source": [
    "> Seems like only few dimensions are useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bedfcb7-c805-440a-8e09-c9f2fcc9dcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Select top predictive dims only\n",
    "top_dims = np.argsort(np.abs(prot_corr_pred))[-10:] \n",
    "prot_selected = prot_emb_arr[:, top_dims]\n",
    "\n",
    "# Concatenate with prediction or ground-truth (as a feature)\n",
    "prot_tsne_input = np.hstack([prot_selected, df_prot_lig['pred'].to_numpy()[:, None]])\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "pts_tsne = tsne.fit_transform(prot_tsne_input)\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.scatterplot(x=pts_tsne[:,0], y=pts_tsne[:,1], hue=df_prot_lig['pred'], palette='viridis')\n",
    "plt.title(\"t-SNE: Protein embeddings + prediction\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf1a3a1-472b-4ae2-8d73-02515b938cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "sns.scatterplot(x=pts_tsne[:,0], y=pts_tsne[:,1], hue=df_prot_lig['seq'], palette='coolwarm')\n",
    "plt.legend('')\n",
    "plt.title(\"t-SNE: Protein embeddings + prediction\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fd8b6c-b0f2-41db-9f2a-c7773301c681",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_dims_lig = np.argsort(np.abs(lig_corr_pred))[-10:]\n",
    "lig_selected = lig_emb_arr[:, top_dims_lig]\n",
    "lig_tsne_input = np.hstack([lig_selected, df_prot_lig['pred'].to_numpy()[:, None]])\n",
    "\n",
    "pts_tsne_lig = TSNE(n_components=2, perplexity=30, random_state=42).fit_transform(lig_tsne_input)\n",
    "\n",
    "sns.scatterplot(x=pts_tsne_lig[:,0], y=pts_tsne_lig[:,1], hue=df_prot_lig['pred'], palette='viridis')\n",
    "plt.title(\"t-SNE: Ligand embeddings + prediction\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71f906b-91bf-4cb2-9315-4bd7fafa4bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=pts_tsne_lig[:,0], y=pts_tsne_lig[:,1], hue=df_prot_lig['num_C'], palette='viridis')\n",
    "plt.title(\"t-SNE: Ligand embeddings + prediction\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3cf897-8228-454c-959c-75ac360603f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_top_dims = np.hstack([prot_selected, lig_selected, df_prot_lig['pred'].to_numpy()[:, None]])\n",
    "pts_tsne_comb = TSNE(n_components=2, perplexity=30, random_state=42).fit_transform(combined_top_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a766e18c-f2df-480f-bc28-589683e919c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=pts_tsne_comb[:,0], y=pts_tsne_comb[:,1], hue=df_prot_lig['pred'], palette='viridis')\n",
    "plt.title(\"t-SNE: Top protein+ligand dims + prediction\")\n",
    "plt.show()\n",
    "sns.scatterplot(x=pts_tsne_comb[:,0], y=pts_tsne_comb[:,1], hue=df_prot_lig['gt'], palette='viridis')\n",
    "plt.title(\"t-SNE: Top protein+ligand dims + prediction\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73d9ea2-bc95-44c4-92a0-d5c464c5d766",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_pts = pts_tsne_comb   # shape (N,2)\n",
    "df_tsne = pd.DataFrame({\n",
    "    'x': tsne_pts[:,0],\n",
    "    'y': tsne_pts[:,1],\n",
    "    'pred': df_prot_lig['pred'],\n",
    "    'gt': df_prot_lig['gt'],\n",
    "    'seq_len': df_prot_lig['target_sequence'].str.len(),\n",
    "    'num_C': df_prot_lig['compound_iso_smiles'].apply(lambda s: s.lower().count('c')),\n",
    "    'num_O': df_prot_lig['compound_iso_smiles'].apply(lambda s: s.lower().count('o')),\n",
    "    'num_N': df_prot_lig['compound_iso_smiles'].apply(lambda s: s.lower().count('n')),\n",
    "    'num_H': df_prot_lig['compound_iso_smiles'].apply(lambda s: s.lower().count('h')),\n",
    "    'num_F': df_prot_lig['compound_iso_smiles'].apply(lambda s: s.lower().count('f')),\n",
    "    'seq': df_prot_lig['target_sequence']\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.scatterplot(data=df_tsne, x='x', y='y', hue='pred', palette='viridis', size='seq_len', sizes=(20,200))\n",
    "plt.title(\"t-SNE colored by prediction, size = sequence length\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.show()\n",
    "\n",
    "for a in 'CON':\n",
    "    plt.figure(figsize=(6,5))\n",
    "    sns.scatterplot(data=df_tsne, x='x', y='y', hue=f'num_{a}', palette='coolwarm')\n",
    "    plt.title(f\"t-SNE colored by number of {a} in ligand\")\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.scatterplot(data=df_tsne, x='x', y='y', hue='seq', palette='coolwarm')\n",
    "plt.title(\"t-SNE colored by sequence\")\n",
    "plt.legend('') \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720d9ce0-22e9-4c3f-90e8-f7b1a19bea59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea16f42-e08a-4a59-9189-576b9f7b88f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_top_dims = np.hstack([prot_selected, lig_selected, df_prot_lig['pred'].to_numpy()[:, None]])\n",
    "pca = PCA(n_components=2)\n",
    "pts_pca_comb = pca.fit_transform(combined_top_dims)\n",
    "sns.scatterplot(x=pts_pca_comb[:,0], y=pts_pca_comb[:,1], hue=df_prot_lig['pred'], palette='viridis')\n",
    "plt.title(\"PCA: Top protein+ligand dims + prediction\")\n",
    "plt.show()\n",
    "sns.scatterplot(x=pts_pca_comb[:,0], y=pts_pca_comb[:,1], hue=df_prot_lig['gt'], palette='viridis')\n",
    "plt.title(\"PCA: Top protein+ligand dims + prediction\")\n",
    "plt.show()\n",
    "sns.scatterplot(x=pts_pca_comb[:,0], y=pts_pca_comb[:,1], hue=df_prot_lig['seq_len'], palette='coolwarm')\n",
    "plt.title(\"PCA: Top protein+ligand dims + prediction\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15feee26-9f1f-4b75-b234-e2fcb1faf735",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import Series\n",
    "import numpy as np\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pts_pca_comb = pca.fit_transform(combined_top_dims)\n",
    "\n",
    "# Loadings\n",
    "loadings = pca.components_  # shape (2, n_dims)\n",
    "n_prot = prot_selected.shape[1]\n",
    "n_lig  = lig_selected.shape[1]\n",
    "\n",
    "# PC1 loadings\n",
    "pc1_loadings = Series(loadings[0], index=[f\"P{i}\" for i in range(n_prot)] +\n",
    "                                      [f\"L{i}\" for i in range(n_lig)] +\n",
    "                                      ['pred'])\n",
    "pc2_loadings = Series(loadings[1], index=[f\"P{i}\" for i in range(n_prot)] +\n",
    "                                      [f\"L{i}\" for i in range(n_lig)] +\n",
    "                                      ['pred'])\n",
    "\n",
    "print(\"Top positive PC1 dims:\")\n",
    "print(pc1_loadings.nlargest(10))\n",
    "print(\"\\nTop negative PC1 dims:\")\n",
    "print(pc1_loadings.nsmallest(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539526bd-7d54-4ea0-9fe8-55ee44e2e2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc1 = pts_pca_comb[:,0]\n",
    "pc2 = pts_pca_comb[:,1]\n",
    "\n",
    "df_prot_lig = pd.DataFrame({\n",
    "    'protein_emb': list(protein_embeddings),  # Each element is a 96-dim array\n",
    "    'ligand_emb': list(ligand_embeddings),    # Each element is a 96-dim array\n",
    "    'pred': predictions,\n",
    "    'gt': ground_truth,\n",
    "    'target_sequence':data_raw.loc[indices, 'target_sequence'].to_numpy(),\n",
    "    'compound_iso_smiles':data_raw.loc[indices, 'compound_iso_smiles'].to_numpy(),\n",
    "    'seq_len': df_prot_lig['target_sequence'].str.len(),\n",
    "    'num_C': df_prot_lig['compound_iso_smiles'].apply(lambda s: s.lower().count('c')),\n",
    "    'num_O': df_prot_lig['compound_iso_smiles'].apply(lambda s: s.lower().count('o')),\n",
    "    'num_N': df_prot_lig['compound_iso_smiles'].apply(lambda s: s.lower().count('n')),\n",
    "    'num_H': df_prot_lig['compound_iso_smiles'].apply(lambda s: s.lower().count('h')),\n",
    "    'num_F': df_prot_lig['compound_iso_smiles'].apply(lambda s: s.lower().count('f')),\n",
    "    'seq': df_prot_lig['target_sequence']\n",
    "})\n",
    "\n",
    "for feature in ['pred','gt','seq_len','num_C','num_O','num_N']:\n",
    "    corr_x = np.corrcoef(pc1, df_prot_lig[feature])[0,1]\n",
    "    corr_y = np.corrcoef(pc2, df_prot_lig[feature])[0,1]\n",
    "    print(f\"{feature}: corr with PC1={corr_x:.2f}, PC2={corr_y:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b686e789-40f6-461b-ad8e-d3470264bda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "sns.barplot(x=pc1_loadings.index, y=pc1_loadings.values)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"PCA PC1 loadings (contribution of each dimension)\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "sns.barplot(x=pc2_loadings.index, y=pc2_loadings.values)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"PCA PC2 loadings (contribution of each dimension)\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
